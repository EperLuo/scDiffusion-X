encoder_kwargs:
  rna:
    dims: [512, 300, 100]   # encoder and decoder have three mlp layers
    norm: True              # wether ues normalization in the mlp layers
    norm_type: batchnorm    # normalize type. use batchnorm or layernorm
    dropout: False 
    dropout_p: 0.0
  atac:
    dims: [1024, 512, 100]
    norm: True
    norm_type: batchnorm
    dropout: False 
    dropout_p: 0.0    
learning_rate: 0.001
weight_decay: 0.0001
covariate_specific_theta: False 
multimodal: True
is_binarized: True
encoder_multimodal_joint_layers: null